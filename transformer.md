# Transformer 模型详解

本文档旨在以浅显易懂的方式讲解 Transformer 模型的核心概念。

### 1. 什么是 Transformer？

在 Transformer 出现之前，处理序列数据（比如文本）的主流模型是循环神经网络（RNN）和长短期记忆网络（LSTM）。这些模型的核心思想是"顺序处理"，一次处理一个单词，并将信息传递给下一个。这种方式有两个主要问题：

1.  **难以并行计算**：因为必须按顺序处理，所以无法利用现代 GPU 强大的并行计算能力，导致训练速度慢。
2.  **长距离依赖问题**：信息在序列中传递时，可能会逐渐丢失，导致模型难以捕捉相距较远的词之间的关系。

2017年，Google 的一篇论文《Attention Is All You Need》提出了 Transformer 模型，彻底改变了这一现状。它完全抛弃了 RNN 的循环结构，仅使用一种叫做 **"注意力机制"（Attention Mechanism）** 的东西来处理序列数据。

**核心优势**：
*   **并行计算能力强**：可以同时处理整个句子的所有单词，训练速度极快。
*   **捕捉长距离依赖能力强**：任意两个单词之间的距离都为1，可以直接计算它们之间的关系。

Transformer 的结构主要分为两大部分：**编码器（Encoder）** 和 **解码器（Decoder）**。

![Encoder-Decoder](https://www.researchgate.net/profile/Yuto-Nishida/publication/359055812/figure/fig2/AS:1134268129759235@1647432726058/The-Transformer-encoder-decoder-architecture-Vaswani-et-al-2017-The-left-and.png)

### 2. 数据准备：词嵌入（Word Embedding）与位置编码（Positional Encoding）

计算机不理解文字，只理解数字。所以第一步，我们需要把输入的单词转换成向量。

**a. 词嵌入 (Word Embedding)**

这一步是将每个单词映射到一个固定长度的向量。例如，我们有一个 10000 个词的词汇表，我们可以把每个词都映射成一个 512 维的向量。这个向量就代表了该词的语义信息。

*   **例子**：
    `"我 是 学生"` -> `[vector_我, vector_是, vector_学生]`
    其中 `vector_我` 可能长这样：`[0.1, 0.9, 0.2, ... , 0.5]` (长度为512)

**b. 位置编码 (Positional Encoding)**

因为 Transformer 同时处理所有单词，它本身无法感知单词的顺序。如果没有位置信息，"我打你" 和 "你打我" 在模型看来可能就没区别了。

为了解决这个问题，模型引入了 **位置编码**。它是一个和词嵌入向量维度相同的向量，通过 `sin` 和 `cos` 函数计算得出。这个位置编码向量会被加到词嵌入向量上，从而为模型注入了单词的位置信息。

`最终输入向量 = 词嵌入向量 + 位置编码向量`

### 3. 核心机制：自注意力机制 (Self-Attention)

这是 Transformer 最核心的部分。一句话概括：**计算句子中每个单词对于其他所有单词的"关注度"或"重要性"**。

为了计算这个关注度，自注意力机制为每个输入向量（我们上面得到的"最终输入向量"）创建三个新的向量：

*   **Query (Q)**: 查询向量，可以理解为"为了更好地理解当前单词，我需要什么样的信息？"
*   **Key (K)**: 键向量，可以理解为"我这个单词能提供什么样的信息？"
*   **Value (V)**: 值向量，是该单词实际包含的信息。

**计算过程（以计算单词 "Thinking" 的注意力为例）**

假设我们的输入是 "Thinking Machines"。

1.  **获取 Q, K, V**：
    我们将 "Thinking" 的输入向量乘以三个不同的权重矩阵 `Wq`, `Wk`, `Wv`（这三个矩阵是模型在训练中学习到的），得到 `q1`, `k1`, `v1`。
    同样，我们对 "Machines" 的输入向量做同样操作，得到 `q2`, `k2`, `v2`。

2.  **计算分数 (Score)**：
    我们用 "Thinking" 的查询向量 `q1` 去和所有单词的键向量 `k` 做点积运算，得到一个分数。这个分数代表 "Thinking" 这个词应该对其他词（包括它自己）投入多少关注。
    *   `Score(Thinking, Thinking) = q1 · k1`
    *   `Score(Thinking, Machines) = q1 · k2`

3.  **缩放 (Scale)**：
    为了让训练过程更稳定，我们会将得到的分数除以一个缩放因子，通常是键向量维度 `dk` 的平方根。`scaled_score = score / sqrt(dk)`。

4.  **Softmax 归一化**：
    将缩放后的分数输入到 Softmax 函数中，得到一组和为 1 的权重。这组权重就是 "Thinking" 对所有单词的"注意力权重"。
    *   `Attention_Weights = softmax([scaled_score_1, scaled_score_2, ...])`

5.  **加权求和**：
    将每个单词的值向量 `v` 与其对应的注意力权重相乘，然后把它们全部加起来，得到最终的自注意力输出向量 `Z`。
    *   `Z_thinking = (weight_1 * v1) + (weight_2 * v2) + ...`

这个 `Z_thinking` 向量就是一个对 "Thinking" 这个词更深刻的理解，因为它融合了整个句子中所有相关单词的信息。

### 4. 多头注意力机制 (Multi-Head Attention)

一次自注意力计算，只是一种"关注"方式。但我们可能希望模型能从不同角度去理解句子。比如，一个头可能关注"谁做了什么"，另一个头可能关注"动作发生在哪里"。

多头注意力就是把上述的自注意力计算重复多次（比如8次，即8个头）。每个头都有自己独立的 `Wq`, `Wk`, `Wv` 权重矩阵。

这样，对于每个单词，我们都会得到8个不同的输出向量 `Z`。然后，我们将这8个向量拼接起来，再通过一个线性层（乘以一个权重矩阵 `Wo`）将其降维回原始的维度。

这样做的好处是，它允许模型在不同的表示子空间中共同关注来自不同位置的信息。

### 5. 编码器 (Encoder) 和解码器 (Decoder) 的结构

**编码器层 (Encoder Layer)**

一个编码器层由两个主要部分组成：
1.  **多头注意力机制 (Multi-Head Attention)**
2.  **前馈神经网络 (Feed-Forward Network)**

在这两个部分之间和之后，都使用了 **残差连接 (Add)** 和 **层归一化 (Layer Norm)**。
*   **残差连接**：将输入 `x` 直接加到输出 `Sublayer(x)` 上，即 `x + Sublayer(x)`。这有助于防止梯度消失，让模型更容易训练。
*   **层归一化**：对每一层的输出进行归一化，使数据分布更稳定，加速训练。

所以，一个编码器层的完整流程是：
`输入 -> 多头注意力 -> Add & Norm -> 前馈网络 -> Add & Norm -> 输出`

一个完整的编码器是由 N 个（例如6个）这样的编码器层堆叠而成的。

**解码器层 (Decoder Layer)**

解码器层比编码器层多一个部分：
1.  **带掩码的（Masked）多头注意力机制**：这是解码器对自身已经生成的部分进行自注意力计算。这里的"掩码"是关键，它会确保在预测第 `i` 个位置的单词时，只能关注到 `i` 之前已经生成的单词，而不能"偷看"未来的信息。
2.  **编码器-解码器注意力机制 (Encoder-Decoder Attention)**：这是连接编码器和解码器的桥梁。它的 **Q 来自于解码器的上一个子层**，而 **K 和 V 则来自于编码器的最终输出**。这使得解码器在生成每一个单词时，都能"关注"到输入句子的所有部分。
3.  **前馈神经网络 (Feed-Forward Network)**

同样，每个子层之后都有 Add & Norm。一个完整的解码器也是由 N 个解码器层堆叠而成。

### 6. 最终输出

解码器栈的最终输出是一个浮点数向量。如何把它变成一个单词呢？
1.  **线性层 (Linear Layer)**：将解码器输出的向量投影到我们词汇表的大小。如果词汇表有10000个词，这个线性层就会输出一个长度为10000的向量。
2.  **Softmax 层**：将这个长向量转换成一个概率分布，每个值代表对应单词的概率。我们通常会选择概率最高的那个单词作为最终的输出。

### 总结

让我们用一个翻译任务（"I am a student" -> "我是一个学生"）来回顾整个流程：

1.  **编码阶段**：
    *   "I", "am", "a", "student" 经过词嵌入和位置编码，变成向量。
    *   这些向量被送入编码器栈。在每一层编码器中，它们通过自注意力机制和前馈网络，不断深化对原始句子的理解。
    *   编码器最终输出一组能代表整个输入句子信息的 K 和 V 向量。

2.  **解码阶段**：
    *   解码器接收开始符 `<s>`，并对其进行嵌入和位置编码。
    *   在解码器层中，首先通过"带掩码的自注意力"处理 `<s>`。
    *   然后，通过"编码器-解码器注意力"，用 `<s>` 的 Q 向量去匹配编码器输出的 K, V 向量，决定应该关注输入句子的哪个部分（此时应该会高度关注 "I"）。
    *   经过前馈网络，最终通过线性和 Softmax 层，预测出第一个单词 "我"。
    *   接下来，解码器将 "我" 作为新的输入，重复上述过程，预测出 "是"......直到预测出结束符 `</s>` 为止。
